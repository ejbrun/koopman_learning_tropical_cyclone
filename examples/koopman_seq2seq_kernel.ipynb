{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from absl import app\n",
    "# from klearn_tcyclone.training_utils.args import FLAGS, ALL_FLAGS\n",
    "from klearn_tcyclone.training_utils.training_utils import get_default_flag_values\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn\n",
    "from torch.utils import data\n",
    "\n",
    "from klearn_tcyclone.climada.tc_tracks import TCTracks\n",
    "from klearn_tcyclone.data_utils import (\n",
    "    LinearScaler,\n",
    ")\n",
    "from klearn_tcyclone.KNF.modules.eval_metrics import RMSE_TCTracks\n",
    "from klearn_tcyclone.KNF.modules.models import Koopman\n",
    "from klearn_tcyclone.KNF.modules.train_utils import (\n",
    "    eval_epoch_koopman,\n",
    "    train_epoch_koopman,\n",
    ")\n",
    "from klearn_tcyclone.knf_data_utils import TCTrackDataset\n",
    "from klearn_tcyclone.training_utils.training_utils import set_flags\n",
    "from absl import app, flags\n",
    "\n",
    "from klearn_tcyclone.training_utils.training_utils import extend_by_default_flag_values\n",
    "\n",
    "from klearn_tcyclone.koopkernel_seq2seq import KoopmanKernelSeq2Seq, RBFKernel\n",
    "from klearn_tcyclone.koopkernel_seq2seq import KoopKernelLoss, batch_tensor_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set some specific parameters and load default values for all other parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "flag_params = {\n",
    "    # \"seed\": 42,\n",
    "    \"year_range\": [1980, 1988],\n",
    "    # \"batch_size\": 16,\n",
    "    \"num_epochs\": 2,\n",
    "    \"train_output_length\": 1,\n",
    "    \"context_length\": 14,\n",
    "}\n",
    "flag_params[\"input_length\"] = flag_params[\"context_length\"]\n",
    "flag_params = extend_by_default_flag_values(flag_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device cuda\n"
     ]
    }
   ],
   "source": [
    "random.seed(flag_params[\"seed\"])  # python random generator\n",
    "np.random.seed(flag_params[\"seed\"])  # numpy random generator\n",
    "\n",
    "torch.manual_seed(flag_params[\"seed\"])\n",
    "torch.cuda.manual_seed_all(flag_params[\"seed\"])\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "feature_list = [\n",
    "    \"lon\",\n",
    "    \"lat\",\n",
    "    \"max_sustained_wind\",\n",
    "    # \"radius_max_wind\",\n",
    "    # \"radius_oci\",\n",
    "    \"central_pressure\",\n",
    "    \"environmental_pressure\",\n",
    "]\n",
    "\n",
    "# feature_list = [\n",
    "#     \"lon\",\n",
    "#     \"lat\",\n",
    "#     \"max_sustained_wind\",\n",
    "#     \"radius_max_wind\",\n",
    "#     \"radius_oci\",\n",
    "#     \"central_pressure\",\n",
    "#     \"environmental_pressure\",\n",
    "# ]\n",
    "\n",
    "# these are not contained as flags\n",
    "# encoder_hidden_dim = flag_params[\"hidden_dim\"]\n",
    "# decoder_hidden_dim = flag_params[\"hidden_dim\"]\n",
    "# encoder_num_layers = flag_params[\"num_layers\"]\n",
    "# decoder_num_layers = flag_params[\"num_layers\"]\n",
    "\n",
    "output_dim = flag_params[\"input_dim\"]\n",
    "num_feats = len(feature_list)\n",
    "learning_rate = flag_params[\"learning_rate\"]\n",
    "# ---------------\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device\", device)\n",
    "\n",
    "scaler = LinearScaler()\n",
    "eval_metric = RMSE_TCTracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-05 00:58:17,313 - climada.hazard.tc_tracks - WARNING - The cached IBTrACS data set dates from 2023-06-07 23:07:38 (older than 180 days). Very likely, a more recent version is available. Consider manually removing the file /home/ecjb/climada/data/IBTrACS.ALL.v04r00.nc and re-running this function, which will download the most recent version of the IBTrACS data set from the official URL.\n",
      "2025-02-05 01:00:06,789 - climada.hazard.tc_tracks - WARNING - 49 storm events are discarded because no valid wind/pressure values have been found: 1980199N31284, 1980200N25270, 1980204N23287, 1980226N15339, 1980238N16328, ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ecjb/packages/climada_python/climada/hazard/tc_tracks.py:614: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  if ibtracs_ds.dims['storm'] == 0:\n"
     ]
    }
   ],
   "source": [
    "# Datasets\n",
    "tc_tracks = TCTracks.from_ibtracs_netcdf(\n",
    "    provider=\"official\",\n",
    "    year_range=flag_params[\"year_range\"],\n",
    "    basin=\"NA\",\n",
    ")\n",
    "\n",
    "tc_tracks_train, tc_tracks_test = train_test_split(tc_tracks.data, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(73,\n",
       " <xarray.Dataset> Size: 8kB\n",
       " Dimensions:                 (time: 134)\n",
       " Coordinates:\n",
       "   * time                    (time) datetime64[ns] 1kB 1986-08-13T12:00:00 ......\n",
       "     lat                     (time) float32 536B 30.1 30.45 30.8 ... 56.2 56.2\n",
       "     lon                     (time) float32 536B -84.0 -84.0 -84.0 ... 7.0 8.0\n",
       " Data variables:\n",
       "     radius_max_wind         (time) float32 536B 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0\n",
       "     radius_oci              (time) float32 536B 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0\n",
       "     max_sustained_wind      (time) float32 536B 10.0 10.0 10.0 ... 15.0 15.0\n",
       "     central_pressure        (time) float32 536B 1.009e+03 1.01e+03 ... 1.006e+03\n",
       "     environmental_pressure  (time) float64 1kB 1.01e+03 1.01e+03 ... 1.01e+03\n",
       "     time_step               (time) float64 1kB 3.0 3.0 3.0 3.0 ... 3.0 3.0 3.0\n",
       "     basin                   (time) <U2 1kB 'NA' 'NA' 'NA' ... 'NA' 'NA' 'NA'\n",
       " Attributes:\n",
       "     max_sustained_wind_unit:  kn\n",
       "     central_pressure_unit:    mb\n",
       "     orig_event_flag:          True\n",
       "     data_provider:            ibtracs_official\n",
       "     category:                 1\n",
       "     name:                     CHARLEY\n",
       "     sid:                      1986226N30276\n",
       "     id_no:                    1986226030276.0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tc_tracks_train), tc_tracks_train[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from klearn_tcyclone.data_utils import (\n",
    "    load_model,\n",
    "    standardized_context_dataset_from_TCTracks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_lag = 1\n",
    "scaler = LinearScaler()\n",
    "basin = \"NA\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is to feed the tensor_context_dataset into the model. Because for the kernels I need the lookback window and the shifted version of this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lon',\n",
       " 'lat',\n",
       " 'max_sustained_wind',\n",
       " 'central_pressure',\n",
       " 'environmental_pressure']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flag_params[\"context_length\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_context_train_standardized = standardized_context_dataset_from_TCTracks(\n",
    "    tc_tracks_train,\n",
    "    feature_list=feature_list,\n",
    "    scaler=scaler,\n",
    "    context_length=flag_params[\"context_length\"],\n",
    "    time_lag=time_lag,\n",
    "    fit=True,\n",
    "    periodic_shift=True,\n",
    "    basin=basin,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_set = TCTrackDataset(\n",
    "    input_length=flag_params[\"input_length\"],\n",
    "    output_length=flag_params[\"train_output_length\"],\n",
    "    tc_tracks=tc_tracks_train,\n",
    "    feature_list=feature_list,\n",
    "    mode=\"train\",\n",
    "    jumps=flag_params[\"jumps\"],\n",
    "    scaler=scaler,\n",
    "    fit=True,\n",
    ")\n",
    "valid_set = TCTrackDataset(\n",
    "    input_length=flag_params[\"input_length\"],\n",
    "    output_length=flag_params[\"train_output_length\"],\n",
    "    tc_tracks=tc_tracks_train,\n",
    "    feature_list=feature_list,\n",
    "    mode=\"valid\",\n",
    "    jumps=flag_params[\"jumps\"],\n",
    "    scaler=scaler,\n",
    "    fit=False,\n",
    ")\n",
    "test_set = TCTrackDataset(\n",
    "    input_length=flag_params[\"input_length\"],\n",
    "    output_length=flag_params[\"test_output_length\"],\n",
    "    tc_tracks=tc_tracks_test,\n",
    "    feature_list=feature_list,\n",
    "    mode=\"test\",\n",
    "    # jumps=flag_params[\"jumps\"], # jumps not used in test mode\n",
    "    scaler=scaler,\n",
    "    fit=False,\n",
    ")\n",
    "train_loader = data.DataLoader(\n",
    "    train_set, batch_size=flag_params[\"batch_size\"], shuffle=True, num_workers=1\n",
    ")\n",
    "valid_loader = data.DataLoader(\n",
    "    valid_set, batch_size=flag_params[\"batch_size\"], shuffle=True, num_workers=1\n",
    ")\n",
    "test_loader = data.DataLoader(\n",
    "    test_set, batch_size=flag_params[\"batch_size\"], shuffle=False, num_workers=1\n",
    ")\n",
    "\n",
    "if len(train_loader) == 0:\n",
    "    raise Exception(\n",
    "        \"There are likely too few data points in the test set. Try to increase year_range.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check why we have nan values!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14, 5])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "torch.Size([32, 14, 5]) <class 'torch.Tensor'>\n",
      "torch.Size([32, 1, 5]) <class 'torch.Tensor'>\n",
      "tensor([0.7815, 0.8036, 0.8226, 0.8416, 0.8575, 0.8733, 0.8892, 0.9050, 0.9184,\n",
      "        0.9319, 0.9422, 0.9525, 0.9588, 0.9652])\n",
      "tensor([0.9667])\n",
      "\n",
      "1\n",
      "torch.Size([32, 14, 5]) <class 'torch.Tensor'>\n",
      "torch.Size([32, 1, 5]) <class 'torch.Tensor'>\n",
      "tensor([0.0538, 0.0689, 0.0784, 0.0879, 0.0855, 0.0831, 0.0744, 0.0657, 0.0499,\n",
      "        0.0340, 0.0293, 0.0245, 0.0238, 0.0230])\n",
      "tensor([0.0245])\n",
      "\n",
      "2\n",
      "torch.Size([32, 14, 5]) <class 'torch.Tensor'>\n",
      "torch.Size([32, 1, 5]) <class 'torch.Tensor'>\n",
      "tensor([-0.2043, -0.2162, -0.2257, -0.2352, -0.2494, -0.2637, -0.2755, -0.2874,\n",
      "        -0.3017, -0.3159, -0.3294, -0.3428, -0.3555, -0.3682])\n",
      "tensor([-0.3824])\n",
      "\n",
      "3\n",
      "torch.Size([32, 14, 5]) <class 'torch.Tensor'>\n",
      "torch.Size([32, 1, 5]) <class 'torch.Tensor'>\n",
      "tensor([-0.0428, -0.0546, -0.0665, -0.0784, -0.0895, -0.1006, -0.1108, -0.1211,\n",
      "        -0.1314, -0.1417, -0.1520, -0.1623, -0.1734, -0.1845])\n",
      "tensor([-0.1940])\n",
      "\n",
      "4\n",
      "torch.Size([32, 14, 5]) <class 'torch.Tensor'>\n",
      "torch.Size([32, 1, 5]) <class 'torch.Tensor'>\n",
      "tensor([-0.1639, -0.1742, -0.1845, -0.1932, -0.2019, -0.2106, -0.2193, -0.2272,\n",
      "        -0.2352, -0.2423, -0.2494, -0.2581, -0.2668, -0.2747])\n",
      "tensor([-0.2827])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for inps, tgts in train_loader:\n",
    "    if counter < 5:\n",
    "        print(counter)\n",
    "        print(inps.shape, type(inps))\n",
    "        print(tgts.shape, type(inps))\n",
    "        print(inps[0,:,0])\n",
    "        print(tgts[0,:,0])\n",
    "        print()\n",
    "    \n",
    "    counter += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbf = RBFKernel(length_scale=1.0)\n",
    "flag_params[\"koopman_kernel_num_centers\"] = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 5]) torch.Size([100, 5])\n"
     ]
    }
   ],
   "source": [
    "koopkernelmodel = KoopmanKernelSeq2Seq(\n",
    "    kernel=rbf,\n",
    "    input_dim = 1,\n",
    "    input_length = 1,\n",
    "    output_length = 1,\n",
    "    output_dim = 1,\n",
    "    num_steps = 1,\n",
    "    num_nys_centers = flag_params[\"koopman_kernel_num_centers\"],\n",
    "    rng_seed = 42,\n",
    ")\n",
    "\n",
    "koopkernelmodel._initialize_nystrom_data(tensor_context_train_standardized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 13, 5]) torch.Size([32, 13, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 13, 5])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inps = tensor_context_train_standardized[:flag_params[\"batch_size\"]].lookback(13)\n",
    "inps = torch.tensor(tensor_context_train_standardized[:flag_params[\"batch_size\"]].lookback(tensor_context_train_standardized.context_length - 1), dtype=torch.float32).to(\n",
    "    device\n",
    ")\n",
    "target = torch.tensor(tensor_context_train_standardized[:flag_params[\"batch_size\"]].lookback(tensor_context_train_standardized.context_length - 1, slide_by=1), dtype=torch.float32).to(\n",
    "    device\n",
    ")\n",
    "print(inps.shape, target.shape)\n",
    "\n",
    "outs = koopkernelmodel.forward(inps)\n",
    "outs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 103, 13, 5]), torch.Size([32, 103, 13, 5]))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_context_inps, tensor_context_tgts = batch_tensor_context(tensor_context_train_standardized, batch_size=flag_params[\"batch_size\"], flag_params=flag_params)\n",
    "assert torch.all(tensor_context_inps[:,:,1:] == tensor_context_tgts[:,:,:-1])\n",
    "tensor_context_inps.shape, tensor_context_tgts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(koopkernelmodel.parameters(), lr=learning_rate)\n",
    "loss_koopkernel = KoopKernelLoss(koopkernelmodel.nystrom_data_Y, koopkernelmodel._kernel)\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "tb_writer = SummaryWriter('runs/fashion_trainer_{}'.format(timestamp))\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer, step_size=1, gamma=flag_params[\"decay_rate\"]\n",
    ")  # stepwise learning rate decay\n",
    "\n",
    "# def train_one_epoch(epoch_index, tb_writer):\n",
    "def train_one_epoch(epoch_index, tb_writer, tensor_context_inps, tensor_context_tgts):\n",
    "    \"\"\"From https://pytorch.org/tutorials/beginner/introyt/trainingyt.html.\"\"\"\n",
    "\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "\n",
    "    print(range(tensor_context_inps.shape[1]))\n",
    "    for i in range(tensor_context_inps.shape[1]):\n",
    "\n",
    "        # Every data instance is an input + label pair\n",
    "        inputs, labels = tensor_context_inps[:,i], tensor_context_tgts[:,i]\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        outputs = koopkernelmodel(inputs)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = loss_koopkernel(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # print(loss.item())\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        if i % 10 == 9:\n",
    "            last_loss = running_loss / 10 # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            tb_x = epoch_index * tensor_context_inps.shape[1] + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.\n",
    "\n",
    "    \n",
    "    print(optimizer.state_dict()[\"param_groups\"][0])\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range(0, 103)\n",
      "  batch 10 loss: 35990.28666992187\n",
      "  batch 20 loss: 11655.11181640625\n",
      "  batch 30 loss: 3359.58955078125\n",
      "  batch 40 loss: 2076.6636138916015\n",
      "  batch 50 loss: 838.6673706054687\n",
      "  batch 60 loss: 518.437353515625\n",
      "  batch 70 loss: 449.80617065429686\n",
      "  batch 80 loss: 314.98473052978517\n",
      "  batch 90 loss: 261.50956573486326\n",
      "  batch 100 loss: 237.34925689697266\n",
      "{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': None, 'initial_lr': 0.001, 'params': [0]}\n"
     ]
    }
   ],
   "source": [
    "epoch_index = 1\n",
    "train_loss = train_one_epoch(epoch_index, tb_writer, tensor_context_inps, tensor_context_tgts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range(0, 103)\n",
      "  batch 10 loss: 242.28590393066406\n",
      "  batch 20 loss: 257.22379913330076\n",
      "  batch 30 loss: 176.7944793701172\n",
      "  batch 40 loss: 172.2018928527832\n",
      "  batch 50 loss: 198.46255645751953\n",
      "  batch 60 loss: 138.46106719970703\n",
      "  batch 70 loss: 185.59183578491212\n",
      "  batch 80 loss: 136.82908630371094\n",
      "  batch 90 loss: 125.52691192626953\n",
      "  batch 100 loss: 135.94329910278321\n",
      "{'lr': 0.0009000000000000001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': None, 'initial_lr': 0.001, 'params': [0]}\n"
     ]
    }
   ],
   "source": [
    "epoch_index = 1\n",
    "train_loss = train_one_epoch(epoch_index, tb_writer, tensor_context_inps, tensor_context_tgts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
